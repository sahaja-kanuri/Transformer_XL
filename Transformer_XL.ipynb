{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahaja-kanuri/Transformer_XL/blob/main/Transformer_XL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAIhp-o0gpX"
      },
      "source": [
        "# Initializing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJEppC74qmJS"
      },
      "outputs": [],
      "source": [
        "#GPU Info:\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqh9hTvMqmyK"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGTbMZQZgruh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, LayerNormalization, Embedding\n",
        "from keras.layers import Softmax, Activation, Add\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTOntiVdKqMr"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "from einops import rearrange #, repeat, pack, unpack, einsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODVNKlNUVKec"
      },
      "outputs": [],
      "source": [
        "# Install pre-requisites:\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "!pip install sacremoses\n",
        "from transformers import TransfoXLTokenizer\n",
        "import os\n",
        "os.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n",
        "\n",
        "# Load the Dataset:\n",
        "wikitext103 = load_dataset('wikitext', 'wikitext-103-v1')\n",
        "print('The wikitext103 dataset: {wikitext103}')\n",
        "\n",
        "train_wikitext103 = wikitext103['train']['text']\n",
        "validation_wikitext103 = wikitext103['validation']['text']\n",
        "test_wikitext103 = wikitext103['test']['text']\n",
        "print(\"\\n No. of training examples: \", len(train_wikitext103))\n",
        "print(\"\\n No. of validation examples: \", len(validation_wikitext103))\n",
        "print(\"\\n No. of test examples: \", len(test_wikitext103))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI6Nor5GX1UR"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained tokenizer:\n",
        "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl/transfo-xl-wt103',\n",
        "                # padding_side='right', truncation_side='right',\n",
        "                pad_token='pad_token')\n",
        "vocab_size = len(tokenizer)\n",
        "print(f'Vocab size is: {vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTKeZSdGZ380"
      },
      "outputs": [],
      "source": [
        "# Identifying how much of the dataset is empty\n",
        "word_counts = list(map(len, wikitext103['train']['text']))\n",
        "\n",
        "print(\"Percentiles of character count in Wikitext103:\")\n",
        "length_quantiles = [0, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99, 1.0]\n",
        "for percentile, length in zip(length_quantiles, np.quantile(word_counts, length_quantiles)):\n",
        "  print(f\" * {percentile:.1%}: {int(length):,} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omM9QQLhaCr5"
      },
      "outputs": [],
      "source": [
        "wikitext_nonempty = wikitext103.filter(lambda x: len(x['text']) >= 10)\n",
        "wikitext_nonempty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaWpexO68f4j"
      },
      "outputs": [],
      "source": [
        "train_wikitext103 = wikitext_nonempty['train']['text']\n",
        "validation_wikitext103 = wikitext_nonempty['validation']['text']\n",
        "test_wikitext103 = wikitext_nonempty['test']['text']\n",
        "print(\"\\n No. of training examples after filtering: \", len(train_wikitext103))\n",
        "print(\"\\n No. of validation examples after filtering: \", len(validation_wikitext103))\n",
        "print(\"\\n No. of test examples after filtering: \", len(test_wikitext103))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvYF7LuMhxrh"
      },
      "outputs": [],
      "source": [
        "# Initializing Transformer XL model dimensions:\n",
        "# Note that 'vocab_size' has been pre-determined above using the TransfoXLTokenizer\n",
        "\n",
        "max_sequence_length = 32  # maximum sequence length\n",
        "train_batch_size = 16 # no. of sequences to process at once\n",
        "# validation_batch_size = 32\n",
        "num_heads = 4 # no. of attention heads\n",
        "head_dim = 16 # size of each attention head\n",
        "embed_dim = 64 # num_heads * head_dim # embedding dimention or d_model\n",
        "max_iters = 50 # no. of iterations of gradient descent\n",
        "learning_rate = 1e-3 # Adam optimization's learning rate\n",
        "num_layers = 4 # no. of layers of Transformer decoder/block\n",
        "num_buckets = 6 # relating to the logarithmic distribution in relative positional encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oh_spcrir8p"
      },
      "outputs": [],
      "source": [
        "# Tokenizes the dataset:\n",
        "tokenized_text_train = tokenizer(train_wikitext103,\n",
        "                                 truncation=True, max_length=max_sequence_length+1,\n",
        "                                 padding='max_length',\n",
        "                                 return_attention_mask=True,\n",
        "                                 return_overflowing_tokens=True,\n",
        "                                 return_tensors='tf')\n",
        "tokenized_text_validation = tokenizer(validation_wikitext103,\n",
        "                                 truncation=True, max_length=max_sequence_length+1,\n",
        "                                 padding='max_length',\n",
        "                                 return_attention_mask=True,\n",
        "                                 return_overflowing_tokens=True,\n",
        "                                 return_tensors='tf')\n",
        "tokenized_text_test = tokenizer(test_wikitext103,\n",
        "                                 truncation=True, max_length=max_sequence_length+1,\n",
        "                                 padding='max_length',\n",
        "                                 return_attention_mask=True,\n",
        "                                 return_overflowing_tokens=True,\n",
        "                                 return_tensors='tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo8sY_iw0IPQ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok7pMn3A0m0E"
      },
      "source": [
        "## Getting Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7AOIk-_WSeX"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, max_sequence_length, train_batch_size):\n",
        "    '''\n",
        "    Generates a batch of training/validation/test examples by selecting\n",
        "    'train_batch_size' number of sequences at random of length\n",
        "    'max_sequence_length'\n",
        "\n",
        "    Arguments:\n",
        "    split -- 'train' or 'validation' or 'test'\n",
        "    max_sequence_length -- maximum length of the sequence\n",
        "    train_batch_size -- number of sequences in a batch\n",
        "\n",
        "    Outputs:\n",
        "    x -- (train_batch_size, max_sequence_length)\n",
        "    y -- (train_batch_size, max_sequence_length)\n",
        "    '''\n",
        "\n",
        "    # Tokenized dataset:\n",
        "    if split == 'train':\n",
        "        tokenized_data = tokenized_text_train\n",
        "    elif split == 'validation':\n",
        "        tokenized_data = tokenized_text_validation\n",
        "    elif split == 'test':\n",
        "        tokenized_data = tokenized_text_test\n",
        "\n",
        "    # The output tokenized text consists of 0s at the positions where the text\n",
        "    # has been padded to 'max_sequence_length+1':\n",
        "    input_attention_mask = tokenized_data['attention_mask']\n",
        "    tokenized_text = tokenized_data['input_ids']\n",
        "    #shape: (len(data), max_sequence_length+1)\n",
        "    padded_tokenized_text = tf.cast(tf.multiply(tokenized_text,\n",
        "                                    input_attention_mask), dtype=tf.float32)\n",
        "    #shape: (len(data), max_sequence_length+1)\n",
        "\n",
        "    # Creates an array of random numbers of shape (train_batch_size,)\n",
        "    # between 0 (incl.) len(tokenized_data) (excl.)\n",
        "    ix = tf.random.uniform(shape=(train_batch_size,), minval=0,\n",
        "                           maxval=len(tokenized_data), dtype=tf.int32)\n",
        "\n",
        "    # Pick only the rows of the 1st argument given in the 2nd argument, ix\n",
        "    # and off-sets the columns by 1 in y compared to x:\n",
        "    x = tf.gather(padded_tokenized_text[:, :max_sequence_length], ix) #shape: (train_batch_size, max_sequence_length)\n",
        "    y = tf.gather(padded_tokenized_text[:, 1:max_sequence_length+1], ix) #shape: (train_batch_size, max_sequence_length)\n",
        "\n",
        "    return x, y #shape: (train_batch_size, max_sequence_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9JLPb-Oianr"
      },
      "source": [
        "One sentence that is 'max_sequence_length' tokens long which gets split into 'max_sequence_length' number of examples ^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHjd1ojh0whD"
      },
      "source": [
        "## Relative Positional Encoding layer class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHSSk3UKanVK"
      },
      "source": [
        "**Relative Positional Embedding**\n",
        "\n",
        "- Positional embeddings are added to the QK embeddings during attention\n",
        "- Relative position embeddings identify, for each input example, how far away all the other tokens are from a specific token of interest\n",
        "- Instead of giving each token a relative position index of n that is n positions away from our token of interest, T5 relative position \"buckets\" some tokens into the same index\n",
        "- First we create this set of indices. then the indices are matched to an embedding layer of weight values. These values are then added to the QK embeddings during attention. The positional embeddings are trained with the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "musZB4EX3fKW"
      },
      "outputs": [],
      "source": [
        "#1.Construct a relative position matrix\n",
        "#2.For offsets larger than what we want, start to spread offset values logarithmically into a finite amount of buckets. (Past a certian max value, we'll just map everything to one value)\n",
        "#3.Initialize embedding weights that we will assign offset values to\n",
        "#4.Now the relative position matrix is mapped to these weights\n",
        "#5.This matrix gets added to our attention when we perform self-attention. Our self-attention now incorporates, as a piece of information, the relative positions between tokens\n",
        "\n",
        "# Relative Positional Encoding:\n",
        "\n",
        "class relative_positional_encoding_xl(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Constructs a relative position matrix *for one head* to be added to the Attention score\n",
        "\n",
        "    Arguments:\n",
        "        rp_scale -- a normalising scale factor\n",
        "        num_buckets -- default value\n",
        "        num_heads -- default value = 1\n",
        "        rp_max_distance -- default value\n",
        "        sequence_length -- query length/ input sequence length,\n",
        "                           default value = max_sequence_length\n",
        "\n",
        "    Returns:\n",
        "        relative_position_values * self.scale -- shape: (1, num_heads, sequence_length, 2*sequence_length)\n",
        "    '''\n",
        "    def __init__(self, rp_scale, num_buckets=num_buckets, num_heads=1,\n",
        "                 rp_max_distance=max_sequence_length):\n",
        "        super().__init__()\n",
        "        # rp stands for relative position\n",
        "        self.scale = rp_scale\n",
        "        self.num_buckets = num_buckets\n",
        "        self.num_heads = num_heads #=1 for single head of attention, later we concatenate for MHA\n",
        "        self.rp_max_distance = rp_max_distance\n",
        "        self.relative_position_embedding = Embedding(self.num_buckets, self.num_heads)\n",
        "\n",
        "    def relative_position_bucket(self, relative_position_matrix):\n",
        "\n",
        "        # For a decoder model, masking upper triangle after inverting sign:\n",
        "        n = -relative_position_matrix\n",
        "        n = tf.math.maximum(n, tf.zeros_like(n))\n",
        "\n",
        "        # T5 modifications: half of the buckets are for exact increments in position\n",
        "        max_exact = self.num_buckets//2\n",
        "\n",
        "        # MASK 1:\n",
        "        is_small = n < max_exact\n",
        "\n",
        "        # MASK 2:\n",
        "        val_if_large = max_exact + (tf.math.log(tf.cast(n, tf.float32) / max_exact) / tf.math.log(self.rp_max_distance / max_exact) * (self.num_buckets - max_exact) )\n",
        "        val_if_large = tf.cast(val_if_large, dtype=tf.int32)\n",
        "        val_if_large = tf.minimum(val_if_large, tf.fill(tf.shape(val_if_large), self.num_buckets - 1))\n",
        "\n",
        "        return tf.where(is_small, n, val_if_large)\n",
        "\n",
        "    def call(self, sequence_length):\n",
        "        sequence_pos = tf.range(sequence_length)\n",
        "        sequence_pos = tf.reshape(sequence_pos, [sequence_pos.shape[0], 1])\n",
        "        context_pos = tf.range(2*sequence_length)\n",
        "        #context_pos = tf.range((-sequence_length, sequence_length)\n",
        "        rel_pos = context_pos - sequence_pos\n",
        "\n",
        "         # There is no logarithm scaling on the first three, i.e, it stays 3 2 1 0:\n",
        "        rel_pos_indices = self.relative_position_bucket(rel_pos)\n",
        "\n",
        "        # We use the above matrix to index into a positional embedding matrix\n",
        "        # which we initialise randomly to be trained by the model\n",
        "        # which then gets added to the Attention Scores.\n",
        "        # There is one of these for each head\n",
        "        rel_pos_values = self.relative_position_embedding(rel_pos_indices)\n",
        "\n",
        "        # Need to reshape from (sequence, context, heads) -> (batch, heads, sequence, context):\n",
        "        rel_pos_values = rearrange(rel_pos_values, 'i j h -> () h i j')\n",
        "        #equivalently:\n",
        "        #rel_pos_values = tf.expand_dims(tf.transpose(rel_pos_values, perm=[2,0,1]), axis=0)\n",
        "\n",
        "        return rel_pos_values * self.scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDLD6ItH0305"
      },
      "source": [
        "## Attention layer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc2Dj3JNW_6Z"
      },
      "outputs": [],
      "source": [
        "class Attention_Head_xl(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Computes Attention score (weights) for one head of self-attention,\n",
        "    generates output to be concatenated in Multi-Head Attention,\n",
        "    and caches Extended Context\n",
        "\n",
        "    Arguments:\n",
        "        head_size -- Size of each head (=embed_dim/num_heads),\n",
        "\n",
        "        x -- Input tensor shape: (batch_size, seq_length, embed_dim),\n",
        "        relative_positions -- defaults to None for input to the 1st sequence,\n",
        "        shape: (1, num_heads=1, seq_length, seq_length) for input to the 2nd sequence,\n",
        "        and (1, num_heads=1, seq_length, 2*seq_length) for the rest,\n",
        "        extended_context -- Cached keys and values from prior segment (acts as\n",
        "                            memory) of shape: (batch_size, seq_length, 2, head_size)\n",
        "\n",
        "    Returns:\n",
        "        output -- shape: (batch_size, seq_length, head_size)\n",
        "        weights -- shape: (batch_size, num_heads=1, seq_length, 2*seq_length)\n",
        "        extended_context_cache -- shape: (batch_size, seq_length, 2, head_size)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, head_size, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = head_size\n",
        "        self.scale = self.head_size**-0.5\n",
        "\n",
        "        self.query = Dense(self.head_size, use_bias=False)\n",
        "        self.key = Dense(self.head_size, use_bias=False)\n",
        "        self.value = Dense(self.head_size, use_bias=False)\n",
        "\n",
        "        # self.mask = self.add_weight(\n",
        "        #     name='mask',\n",
        "        #     shape=(seq_length, 2*seq_length),\n",
        "        #     initializer=tf.keras.initializers.Constant(\n",
        "        #         tf.linalg.band_part(tf.ones((seq_length, 2*seq_length)), 0, seq_length-1)),\n",
        "        #     trainable=False)\n",
        "\n",
        "        self.layer_norm_q = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layer_norm_k = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, relative_positions=None, extended_context=None,\n",
        "             training=False):\n",
        "        # maybe an unnecesaary step since these are predefined variables:\n",
        "        #batch_size, seq_length, head_size = x.shape # embed_dim = x.shape[2], not head size -- check again and again\n",
        "\n",
        "        # Local Attention:\n",
        "        q = self.query(x) #(batch_size, seq_length, head_size)\n",
        "        k = self.key(x) #(batch_size, seq_length, head_size)\n",
        "        v = self.value(x) #(batch_size, seq_length, head_size)\n",
        "\n",
        "        # To mitigate model drift/ covariant shift:\n",
        "        q = self.layer_norm_q(q)\n",
        "        k = self.layer_norm_k(k)\n",
        "\n",
        "        if extended_context is not None:\n",
        "            # Unpack extended_context and concatentate with keys and values\n",
        "            xl_keys, xl_values = tf.unstack(extended_context, axis=-2)\n",
        "            # shape: (batch_size, seq_length, head_size)\n",
        "\n",
        "            xl_seq_length = xl_keys.shape[1]\n",
        "\n",
        "            # Prepend k and v values along the 'seq_length' axis:\n",
        "            k = tf.concat([xl_keys, k], axis=-2)\n",
        "            v = tf.concat([xl_values, v], axis=-2)\n",
        "            # shape: (batch_size, 2*seq_length, head_size)\n",
        "\n",
        "        # Compute the dot product of attention scores:\n",
        "        weights = tf.linalg.matmul(q, k, transpose_b=True) * self.scale\n",
        "        # if extended_context is not None:\n",
        "        # shape: (batch_size, seq_length, head_size) * (batch_size, head_size, 2*seq_length)\n",
        "        # = (batch_size, seq_length, 2*seq_length)\n",
        "        # the above transposes only the two last axes of the second given tensor\n",
        "\n",
        "        i, j = weights.shape[-2:] # i = seq_length, j = 2*seq_length\n",
        "\n",
        "        # Adding relative positional encodings to the attention weights:\n",
        "        if relative_positions is not None:\n",
        "            weights = tf.expand_dims(weights, axis=1) # weights for each head (axis=1)\n",
        "            #shape: = (batch_size, num_heads=1, seq_length, 2*seq_length)\n",
        "\n",
        "            weights += relative_positions[..., -i:, -j:] # shape of rel_pos: (1, num_heads=1, seq_length, 2*seq_length)\n",
        "            # shape: (batch_size, num_heads=1, seq_length, 2*seq_length)\n",
        "\n",
        "            weights = tf.squeeze(weights, axis=1)\n",
        "            # shape: (batch_size, seq_length, 2*seq_length)\n",
        "\n",
        "\n",
        "        # Creates a look-ahead mask (for a causal language model):\n",
        "        mask = tf.linalg.band_part(tf.ones([i, j], dtype=tf.bool), -1, j-i) == False\n",
        "        weights = tf.where(mask, tf.fill((i, j), float('-inf')), weights)\n",
        "\n",
        "        # Perform softmax along the last axis, i.e. along each row/sequence\n",
        "        weights = Softmax(axis=-1)(weights) # shape: (batch_size, num_heads=1, seq_length, 2*seq_length)\n",
        "        weights = self.dropout(weights, training=training)\n",
        "\n",
        "        # Computes the attention scores (\"affinities\"):\n",
        "        output = tf.linalg.matmul(weights, v)\n",
        "        # (batch_size, seq_length, 2*seq_length) * (batch_size, 2*seq_length, head_size)\n",
        "        # = (batch_size, seq_length, head_size)\n",
        "\n",
        "        # Passing on keys and values:\n",
        "        kv_memory = tf.stack([k,v], axis=-2)\n",
        "        # shape: (batch_size, 2*seq_length, 2, head_size), if extended_context is not None\n",
        "        # But we want (batch_size, seq_length, 2, head_size), which is the case when extended_context is None.\n",
        "        # For this:\n",
        "\n",
        "        if extended_context is not None:\n",
        "            #for all sequences except the first:\n",
        "            xl_memory, current_input = kv_memory[:, :-xl_seq_length, :, :], kv_memory[:, -xl_seq_length:, :, :]\n",
        "            extended_context_cache = current_input\n",
        "            # shape: (batch_size, seq_length, 2, head_size)\n",
        "            # discard xl_memory\n",
        "        else:\n",
        "            #for the first sequence, we don't need to split them out:\n",
        "            extended_context_cache = kv_memory\n",
        "            # shape: (batch_size, seq_length, 2, head_size)\n",
        "\n",
        "        return output, weights, tf.stop_gradient(extended_context_cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE2ELMOl07GI"
      },
      "source": [
        "## Multi-Head Attention layer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbc9_AI6XDjc"
      },
      "outputs": [],
      "source": [
        "class Multi_Head_Attention_xl(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Uses 'Attention_Head' to implement multiple heads of self-attention\n",
        "\n",
        "    Arguments:\n",
        "        head_size -- Size of each head (a type of communication channel, C),\n",
        "        num_heads -- Number of heads,\n",
        "        x -- Input tensor shape: (batch_size, seq_length, embed_dim)\n",
        "        relative_positions_mha -- relative positional encoding matrix\n",
        "        extended_context_mha -- defaults to None, or of shape: (batch_size, seq_length, 2, head_size * num_heads)\n",
        "\n",
        "    Returns:\n",
        "        out -- shape: (batch_size, seq_length, head_size * num_heads=embed_dim),\n",
        "        where head_size * num_heads = embed_dim ,\n",
        "        extended_context_mha -- shape: (batch_size, seq_length, 2, head_size)\n",
        "    '''\n",
        "    def __init__(self, head_size, num_heads, dropout_rate=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = head_size\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = self.head_size * self.num_heads\n",
        "\n",
        "        self.heads = [Attention_Head_xl(self.head_size) for _ in range(self.num_heads)]\n",
        "        # self.head = Attention_Head_xl(self.head_size)\n",
        "        self.projection = Dense(self.embed_dim)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, relative_positions_mha=None, extended_context_mha=None, training=False):\n",
        "        '''\n",
        "        training: boolean, i.e. either 'True' or 'False'\n",
        "        Let extended_context_mha stay None when initialising, since for the\n",
        "        first sequence, there will be no prior context.\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        Alternatively:\n",
        "        for i, head in enumerate(self.heads):\n",
        "            #print(f'i = {i}')\n",
        "            output, _, extended_context_mha = head(x,\n",
        "                                                    relative_positions=relative_positions_mha,\n",
        "                                                    extended_context=extended_context_mha,\n",
        "                                                    training=training)\n",
        "            # shape of output: (batch_size, seq_length, head_size)\n",
        "            out.append(output)\n",
        "        '''\n",
        "        out = []\n",
        "        for i in range(self.num_heads):\n",
        "            # pass x and the encoder output through a stack of decoder layers and\n",
        "            # save the attention weights of block 1 and 2\n",
        "            output, _, extended_context_mha = self.heads[i](x,\n",
        "                                                    relative_positions=relative_positions_mha,\n",
        "                                                    extended_context=extended_context_mha,\n",
        "                                                    training=training)\n",
        "            out.append(output)\n",
        "\n",
        "        out = tf.concat(out, axis=-1)\n",
        "        # shape: (batch_size, seq_length, head_size * num_heads=embed_dim)\n",
        "\n",
        "        out = self.projection(out)\n",
        "        out = self.dropout(out, training=training)\n",
        "\n",
        "        return out, extended_context_mha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "476DxyhL1AYs"
      },
      "source": [
        "## Feed Forward Neural Network layer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKfdZVwKXOU-"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements a feed-forward network\n",
        "\n",
        "    Arguments:\n",
        "        embed_dim -- Size of the input embedding\n",
        "        x -- Input tensor shape: (batch_size, seq_length, head_size * num_heads=embed_dim)\n",
        "\n",
        "    Returns:\n",
        "        output -- shape: (batch_size, seq_length, embed_dim)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        full_connected_dim = 4 * embed_dim\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "                    Dense(full_connected_dim, activation='relu'),\n",
        "                    Dropout(dropout_rate),\n",
        "                    Dense(embed_dim),\n",
        "                    Dropout(dropout_rate)\n",
        "                    ])\n",
        "    def call(self, x, training=False):\n",
        "        return self.ffn(x, training=training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByNK9LB51FN0"
      },
      "source": [
        "## Transformer Decoder layer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egf4XOww6rnz"
      },
      "outputs": [],
      "source": [
        "class Transformer_xl_Decoder(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements the transformer decoder block\n",
        "\n",
        "    Arguments:\n",
        "        num_heads -- Number of heads\n",
        "        head_size -- size of each head\n",
        "        x -- Input tensor shape: (batch_size, seq_length, embed_dim)\n",
        "        rel_pos_decoder -- defaults to None, relative positional encoding matrix\n",
        "        ext_context -- defaults to None, of shape:\n",
        "                        (batch_size, seq_length, 2, head_size * num_heads)\n",
        "\n",
        "    Returns:\n",
        "        output -- shape: (batch_size, seq_length, embed_dim)\n",
        "        ext_context -- shape: (batch_size, seq_length, 2, head_size)\n",
        "    '''\n",
        "    def __init__(self, num_heads, head_size, layernorm_eps=1e-6):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = num_heads * head_size\n",
        "\n",
        "        self.Multi_Head_Attention = Multi_Head_Attention_xl(head_size, num_heads)\n",
        "        self.FeedForward = FeedForward(self.embed_dim)\n",
        "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "    def call(self, x, rel_pos_decoder=None, ext_context=None, training=False):\n",
        "        # Skip connection/ residual connection,\n",
        "        # layer norm is applied before mha, a slight deviation from\n",
        "        # the original Attention paper:\n",
        "\n",
        "        residual = tf.identity(x)\n",
        "        #residual = tf.expand_dims(x, axis=-1)\n",
        "        # shape of residual: (batch_size, seq_length)\n",
        "\n",
        "        x, ext_context = self.Multi_Head_Attention(self.layernorm1(x),\n",
        "                                                relative_positions_mha=rel_pos_decoder,\n",
        "                                                extended_context_mha=ext_context,\n",
        "                                                training=training)\n",
        "        #shape of x: (batch_size, seq_length, head_size * num_heads)\n",
        "\n",
        "        x = Add()([residual, x])\n",
        "\n",
        "        # Skip connection/ residual connection:\n",
        "        x = Add()([x, self.FeedForward(self.layernorm2(x), training=training)])\n",
        "        #shape: (batch_size, seq_length, embed_dim)\n",
        "        return x, ext_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_hcUT7c1J4m"
      },
      "source": [
        "## Transformer XL model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYaktYYZ6xAA"
      },
      "outputs": [],
      "source": [
        "class Transformer_xl(tf.keras.models.Model):\n",
        "    '''\n",
        "    A Model class consisting of all trainable layers, implementing the Transformer\n",
        "    block 'num_layers' number of times\n",
        "\n",
        "    Arguments:\n",
        "        num_layers -- Number of layers of the Transformer Block\n",
        "        embed_dim -- Input embedding dimension\n",
        "        num_heads -- Number of heads in Multi-Head Attention\n",
        "        seq_length -- Maximum sequence length of each input sentence\n",
        "        rp_scale -- a normalising scale factor for the Relative Positional Encoding matrix\n",
        "\n",
        "        idx -- Input training data, shape: (batch_size, seq_length), which is the\n",
        "                output of the 'get_batch' function\n",
        "        ext_context -- defaults to None, of shape:\n",
        "                        (batch_size, seq_length, 2, head_size * num_heads)\n",
        "\n",
        "    Returns:\n",
        "        logits -- shape: (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "    '''\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, seq_length,\n",
        "                rp_scale, layernorm_eps=1e-6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = embed_dim//num_heads\n",
        "        self.rp_scale = rp_scale\n",
        "\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = Embedding(vocab_size, self.embed_dim)\n",
        "        self.position_embedding_table = Embedding(self.seq_length, self.embed_dim)\n",
        "\n",
        "        # Generates the relative positional encoding matrix\n",
        "        self.rel_pos_enc = relative_positional_encoding_xl(self.rp_scale)\n",
        "\n",
        "        self.decoder_layers = [Transformer_xl_Decoder(num_heads= self.num_heads,\n",
        "                                                      head_size = self.head_size)\n",
        "                                for _ in range(self.num_layers)]\n",
        "\n",
        "        self.head = Dense(vocab_size)\n",
        "        self.layernorm = LayerNormalization(epsilon=layernorm_eps) #final layer norm\n",
        "\n",
        "    def call(self, idx, ext_context=None, training=False):\n",
        "        B, T = idx.shape # B= batch_size, T = seq_length\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_dim)\n",
        "        pos_emb = self.position_embedding_table(tf.range(T)) # (T,embed_dim)\n",
        "        x = tok_emb + pos_emb #  (B,T,embed_dim)\n",
        "\n",
        "        rel_pos = self.rel_pos_enc.call(T)\n",
        "        # shape: (1, num_heads, T, 2*T)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # pass x and the encoder output through a stack of decoder layers and\n",
        "            # save the attention weights of block 1 and 2\n",
        "            x, ext_context = self.decoder_layers[i](x, rel_pos_decoder=rel_pos,\n",
        "                                                    ext_context=ext_context, training=training)\n",
        "            # (B,T,embed_dim)\n",
        "\n",
        "        x = self.layernorm(x) # (B,T,embed_dim)\n",
        "        logits = self.head(x) # (B,T,vocab_size)\n",
        "\n",
        "        # B, T, C = logits.shape\n",
        "        # logits = logits.view(B*T, C)\n",
        "        # targets = targets.view(B*T)\n",
        "        # loss = CategoricalCrossentropy()(targets, logits)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        '''\n",
        "        Generates 'max_new_tokens' number of tokens for an input, 'idx'\n",
        "        '''\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -self.seq_length:]\n",
        "            # get the predictions\n",
        "            logits = self(idx_cond) #logits, loss previously\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = Softmax(axis=-1)(logits) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = tf.random.categorical(logits=probs,  num_samples=1) # (B, 1)\n",
        "            idx_next = tf.cast(idx_next, tf.int32)\n",
        "            #input to tf.concat must be of type int32\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = tf.concat([idx, idx_next], axis=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEIqj9uz1eXX"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VURZfUVp6zPV"
      },
      "outputs": [],
      "source": [
        "model = Transformer_xl(num_layers=num_layers, embed_dim=embed_dim, num_heads=num_heads,\n",
        "                       seq_length=max_sequence_length, rp_scale = 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNu5sWcUdbwN"
      },
      "outputs": [],
      "source": [
        "xb, yb = get_batch('train', max_sequence_length=max_sequence_length,\n",
        "                       train_batch_size=train_batch_size)\n",
        "modeltemp = model(xb)\n",
        "print(f'Model output logits shape: {modeltemp.shape}')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-F0RdyS63wC"
      },
      "outputs": [],
      "source": [
        "for iter in range(max_iters):\n",
        "    # Sample a batch of data for training:\n",
        "    xb, yb = get_batch('train', max_sequence_length=max_sequence_length,\n",
        "                       train_batch_size=train_batch_size)\n",
        "\n",
        "    # Evaluate the loss\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss=SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "    print(f'{iter+1}. Training:')\n",
        "    # Fit the model to the training data:\n",
        "    model.fit(xb, yb)\n",
        "\n",
        "    # Sample a batch of data for testing:\n",
        "    xb_valid, yb_valid = get_batch('validation', max_sequence_length=max_sequence_length,\n",
        "                       train_batch_size=train_batch_size)\n",
        "\n",
        "    print(f'Validation:')\n",
        "    loss_validation = model.evaluate(xb_valid, yb_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXqi9sGpMJTA"
      },
      "outputs": [],
      "source": [
        "context = tf.zeros((1, 1), dtype=tf.int32)\n",
        "# prediction = model.predict(context)\n",
        "# print(prediction.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2ZwT9cEMOjL"
      },
      "outputs": [],
      "source": [
        "# generate from the model\n",
        "generated_tokens = model.generate(context, max_new_tokens=100)\n",
        "print(tokenizer.decode(generated_tokens[0].numpy().tolist(), skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o74AMvmV9n_k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMZrOV+gM3pJIvyuG4J6P3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}